2.1

Visulaizing the vanishing gradients probelem by initializing with the weights with various mean, SD for sigmoid, tanh and ReLU activation functions.
Trying out Xavier Initialization technique to check if it mitigates vanishing gradients probelem. (problem solved for sigmoid and tanh activation functions).
Trying out the He Initialization technique to check if it mitigates vanishing gradients probelem for ReLU activation function.

2.2 and 2.3

Experimenting with Leaky ReLU and how it helps mitigating the Dead Neurons problem.
Performed 1000 simulations of NYU HPC. Found that about 30% of simulations result in neural colapse when ReLU is used and 0% of simulations result in neural colapse when Leaky ReLU is used.
